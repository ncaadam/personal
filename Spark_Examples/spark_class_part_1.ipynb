{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Apache Spark Class: Part 1 - Introduction\n",
    "In the last section, we covered the steps needed to install Spark, if you haven't done that then do that now before moving forward.\n",
    "\n",
    "In this section we cover the following:<br>\n",
    "1. What is Apache Spark?\n",
    "2. Importing the Spark Context\n",
    "3. Brief intro to map and reduce with wordcounts\n",
    "4. Explaination of transformation and action functions\n",
    "5. Explaining lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is Apache Spark?\n",
    "Apache Spark is a distributed computing framework that can handle many types of large-scale data processing tasks.  \n",
    "\n",
    "In a very short time, Apache Spark has emerged as the next generation big data processing engine, and is being applied in production environemts as a replacement for many first generation \"Big Data\" tools. Spark improves over Hadoop MapReduce in several key dimensions:\n",
    "* it is much faster due to its ability to store data in memory; some tasks can be sped up more than 100x \n",
    "* much easier to use due to its rich Java, Scala, and Python APIs \n",
    "* it goes far beyond batch applications to support a variety of workloads, including interactive queries, streaming, machine learning, and graph processing\n",
    "\n",
    "Spark has a core processing engine, as well as four main libraries:<br>\n",
    "<img src=\"https://spark.apache.org/images/spark-stack.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Spark Context\n",
    "The Spark Context is the root of the Spark Python API.  To access it in the notebook, we need to import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Configure Spark Settings\n",
    "The SparkConf() class is the main class used for setting up our Spark app.  If you hit tab after conf., you will see a list of all settings available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x106dbe1d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf=SparkConf()\n",
    "conf.setAppName(\"My App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initialize SparkContext\n",
    "Once we have configurations set, we can create a spark context using the SparkContext class. Note that the 'local[*]' argument means that we are executing the code on our local machine and the * means use all available threads for computation (very important to include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]', conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now also go to http://localhost:4040/jobs/ and see the Spark App that you set up along with the progress of any jobs running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Printing the spark context shows that spark is connected to our notebook.  It should print something like:\n",
    "```\n",
    "<pyspark.context.SparkContext object at 0x7ffcaa3204d0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7ffcaa3204d0>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc. #hitting tab when the cursor is after the period to see what functions are available with the context, try it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Brief intro to map and reduce with wordcounts\n",
    "Let's do a quick example to get used to using the sc.  In this case we want to get a count for how many times each word in the README file is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Create an RDD\n",
    "To make an RDD (Resilient Distributed Dataset, a spark data structure) called readme, read in a text file in the spark directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#readme = sc.textFile('/home/<YOUR USERNAME>/spark/spark-1.3.0/README.md') #change <YOUR USERNAME> to your actual un\n",
    "readme = sc.textFile('/home/scott/Documents/spark-1.3.0/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readme. # hitting tab when the cursor is after the period will reveal all of the functions we can use on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these mostly break down into two types: transformation and action functions, but we will dive into that later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###See what's there\n",
    "Let's do a simple count the number of lines in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at what the first few lines look like in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, and Python, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is a \"list-like\" representation of the first five lines of the text file.  Note this not actually a list, it is an RDD so we will have to treat it like one using the functions available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Counting the words\n",
    "Now let's do our first manipulation of the RDD using map and a lambda function.  If you are not familiar with lambda functions, now would be a good time to read about them: https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/ Essentially what we are doing here, is saying: for each line in the RDD, execute (the map part) the function split() which just splits the string at each space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_lines = readme.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the result, we can see it worked and we now have each row in the RDD split into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#', u'Apache', u'Spark'],\n",
       " [],\n",
       " [u'Spark',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'fast',\n",
       "  u'and',\n",
       "  u'general',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'system',\n",
       "  u'for',\n",
       "  u'Big',\n",
       "  u'Data.',\n",
       "  u'It',\n",
       "  u'provides']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there is one problem: we want all the words to be on their own individual row, and not one line per line in the file.  Fortunately, spark has something called flatMap, which is very similar to map, but does a final operation that takes the result of split() and puts each word from each line into its own row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = readme.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'a',\n",
       " u'fast',\n",
       " u'and',\n",
       " u'general',\n",
       " u'cluster']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see it worked; let's just do a quick double check by counting how many words there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the somewhat tricky part: we want to count how many times each word appears.  To do this we will first take each word, and put it in a tuple that looks like: (word, 1) where the 1 is just a static placeholder integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#', 1), (u'Apache', 1), (u'Spark', 1), (u'Spark', 1), (u'is', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the secret sauce: reduceByKey.  We will now try and add up all the words to get a total count because we have things stored in a key, value format (word, 1), we can combine all the keys (aka words) that are the same while also adding thier values we do this using a lambda function with two arguments x and y which represent the key's current value plus the new value.  For example if we are mid way through adding the values and have (Apache, 3) and want to add one more (Apache, 1) we would end up with x = 3, y = 1 and the result is (Apache, 3+1) or (Apache, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = wordcounts.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'all', 1),\n",
       " (u'when', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result so far looks good, but we also want to see the words sorted by top counts to do this we will map a function that switches places for the word and count and then chain that with another function that sorts it in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = wordcounts.map(lambda x:(x[1],x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, u'the'),\n",
       " (14, u'Spark'),\n",
       " (14, u'to'),\n",
       " (11, u'for'),\n",
       " (10, u'and'),\n",
       " (9, u'a'),\n",
       " (8, u'##'),\n",
       " (7, u'run'),\n",
       " (6, u'is'),\n",
       " (6, u'on')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Chaining Transformations\n",
    "The result looks pretty good! 'the' is the most frequent word one thing to note in that last map and sort is that we can chain transformations in one big list of things to do and get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = readme.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, u'the'),\n",
       " (14, u'Spark'),\n",
       " (14, u'to'),\n",
       " (11, u'for'),\n",
       " (10, u'and'),\n",
       " (9, u'a'),\n",
       " (8, u'##'),\n",
       " (7, u'run'),\n",
       " (6, u'is'),\n",
       " (6, u'on')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you did your first analysis using Spark!\n",
    "<img src=\"https://s3.amazonaws.com/lowres.cartoonstock.com/education-teaching-word_count-essay-expression-idiom-phrase-kwan201_low.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Explaination of transformation and action functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "As we’ve discussed, RDDs support two types of operations: transformations and actions. Transformations are operations on RDDs that return a new RDD, such as map() and filter(). Actions are operations that return a result to the driver program or write it to storage, and kick off a computation, such as count() and first(). Spark treats transformations and actions very differently, so understanding which type of operation you are performing will be important. If you are ever confused whether a given function is a transformation or an action, you can look at its return type: transformations return RDDs, whereas actions return some other data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Transformations\n",
    "Transformations are operations on RDDs that return a new RDD. Transformed RDDs are computed lazily, only when you use them in an action (more on that later). Many transformations are element-wise; that is, they work on one element at a time; but this is not true for all transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of some of the most interesting or useful transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_rdd = sc.parallelize([1,1,2,3,5,8,11,19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'a'), (2, 'a'), (3, 'a'), (4, 'a'), (6, 'a')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map - Apply a function to each element in the RDD and return an RDD of the result.\n",
    "map_example = example_rdd.map(lambda x: (x + 1, 'a'))\n",
    "map_example.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '2', '3', '4', '6']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap - Apply a function to each element in the RDD and return an RDD of the contents \n",
    "#of the iterators returned. Often used to extract words.\n",
    "flatmap_example = map_example.flatMap(lambda x: str(x[0]))\n",
    "flatmap_example.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter - Return an RDD consisting of only elements that pass the condition passed tofilter().\n",
    "filter_example = example_rdd.filter(lambda x: x % 2 == 0)\n",
    "filter_example.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1, 5, 2, 19, 11, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distinct - Remove duplicates.\n",
    "distinct_example = example_rdd.distinct()\n",
    "distinct_example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 11, 19]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample - Sample an RDD, with or without replacement.\n",
    "sample_example = example_rdd.sample(False, 0.4)\n",
    "sample_example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 5, 8, 11, 19, 8, 1, 5, 2, 19, 11, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#union - Produce an RDD containing elements from both RDDs.\n",
    "union_example = example_rdd.union(distinct_example)\n",
    "union_example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersection\n",
    "intersection_example = filter_example.intersection(union_example)\n",
    "intersection_example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 3, 3, 5, 5, 11, 11, 19, 19]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtract\n",
    "subtract_example = union_example.subtract(intersection_example)\n",
    "subtract_example.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Actions\n",
    "We’ve seen how to create RDDs from each other with transformations, but at some point, we’ll want to actually do something with our dataset. Actions are the second type of RDD operation. They are the operations that return a final value to the driver program or write data to an external storage system. Actions force the evaluation of \n",
    "the transformations required for the RDD they were called on, since they need to actually produce output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take - Return all elements from the RDD.\n",
    "example_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count - Number of elements in the RDD.\n",
    "example_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {(9, 'a'): 1, (12, 'a'): 1, (3, 'a'): 1, (6, 'a'): 1, (20, 'a'): 1, (2, 'a'): 2, (4, 'a'): 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByValue - Number of times each element occurs in the RDD.\n",
    "map_example.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take - Return num elements from the RDD.\n",
    "example_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 11, 8, 5, 3, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top - Return the top num elements the RDD.\n",
    "example_rdd.top(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 11, 8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeOrdered(num)(ordering) - Return num elements based on provided ordering.\n",
    "example_rdd.takeOrdered(3, lambda x: -x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeSample - Return num elements at random.\n",
    "example_rdd.takeSample(False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce - Combine the elements of the RDD together in parallel (e.g.,sum).\n",
    "example_rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fold - Same as reduce() but with the provided zero value.\n",
    "example_rdd.fold(0, lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregate - Similar to reduce() but used to return a different type.\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "example_rdd.aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#foreach - Apply the provided function to each element of the RDD.\n",
    "def f(x): print x\n",
    "example_rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Explaining lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lazy Evaluation\n",
    "<p>Transformations on RDDs are lazily evaluated, meaning that Spark will not begin to execute until it sees an action. This can be somewhat counter intuitive for new users, but may be familiar for those who have used functional languages such as Haskell or LINQ-like data processing frameworks.</p>\n",
    "\n",
    "<p>Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call sc.textFile(), the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can\n",
    "occur multiple times.</p>\n",
    "\n",
    "<p>Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). This is an easy way to test out just part of your program.</p>\n",
    "\n",
    "<p>Spark uses lazy evaluation to reduce the number of passes it has to take over our data by grouping operations together. In systems like Hadoop MapReduce, developers often have to spend a lot of time considering how to group together operations to minimize the number of MapReduce passes. In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into smaller, more manageable operations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lazy_example = example_rdd.map(lambda x: x + 5)\n",
    "#nothing actually happends here; Spark is waiting for an action so that it \n",
    "#can evaluate things in the most efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_example.count()\n",
    "# things happen now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
