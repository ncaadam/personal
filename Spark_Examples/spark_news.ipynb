{
 "metadata": {
  "name": "",
  "signature": "sha256:3d50674cc9b0f237133477fcdf30e1628b35e4b19b2fb35054345e5a55dea4ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example article tokenizer and word/article count\n",
      "This notebook uses Spark to quickly tokenize and aggreate the words of a large set of news articles.  The source of the data is here: https://www.cs.washington.edu/node/9473/, it contains approximately 500,000 articles from 2013"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import libraries and set spark configurations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import ast\n",
      "from gensim import corpora, models, similarities\n",
      "from pyspark import SparkContext, SparkConf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Configure Spark Settings\n",
      "conf=SparkConf()\n",
      "conf.set(\"spark.executor.memory\", \"2g\")\n",
      "#conf.set(\"spark.cores.max\", \"4\")\n",
      "conf.setAppName(\"News Analysis\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<pyspark.conf.SparkConf at 0x7f8a300ae050>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize SparkContext\n",
      "# it's important here that we put the [*] which indicates spark should use all available resouces\n",
      "sc = SparkContext('local[*]', conf=conf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<pyspark.context.SparkContext object at 0x7f8a300a0f90>\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tokenization and cleanup\n",
      "Read in the data set and do a quick count to make sure evertything is there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the data set\n",
      "raw_json = sc.textFile('/home/scott/Documents/data/release/crawl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_json.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "546713"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up the function that will take in an article and tokenize, remove stop words, and do some other cleanup.  This can be changed/adapted to future uses (e.g., other types of cleanup or article counts)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = nltk.corpus.stopwords.words('english')\n",
      "def tokenize_article(line):\n",
      "    try:\n",
      "        article_dict = ast.literal_eval(line)\n",
      "        if 'text' in article_dict:\n",
      "            article_text = article_dict['text']\n",
      "            article_text = article_text.decode('ascii', 'ignore')\n",
      "            #article_tokens = nltk.word_tokenize(article_text) #pulled this out in favor of the one below, but leaving here for now\n",
      "            tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
      "            article_tokens = tokenizer.tokenize(article_text)\n",
      "            article_tokens = [word for word in article_tokens if word.lower() not in stoplist]\n",
      "            #TODO: investigate nltk.pos_tag\n",
      "            return article_tokens\n",
      "        else:\n",
      "            return []\n",
      "    except:\n",
      "        print 'failed to process article'\n",
      "        print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_articles = raw_json.map(lambda line: tokenize_article(line))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save as a text file for later in case we want to start at this point\n",
      "tokenized_articles.saveAsTextFile('/home/scott/Documents/data/release/crawl_tokenized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do a word count across all articles.  This takes about 8 minutes on a quad core i5 with 16 GB RAM.  If you go to http://localhost:4040/stages/ you can see the progress."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_counts = tokenized_articles.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1,623,175 without lower\n",
      "word_counts.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "1622787"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_counts.take(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[(2473832, u'u0027s'),\n",
        " (1688436, u'said'),\n",
        " (760835, u'year'),\n",
        " (729743, u'one'),\n",
        " (715197, u'2013'),\n",
        " (651169, u'would'),\n",
        " (625836, u'1'),\n",
        " (620594, u'u0027t'),\n",
        " (584655, u'also'),\n",
        " (556650, u'time'),\n",
        " (537764, u'first'),\n",
        " (515065, u'u0027'),\n",
        " (511790, u'two'),\n",
        " (494106, u'people'),\n",
        " (482071, u'last'),\n",
        " (478850, u'new'),\n",
        " (477373, u'2'),\n",
        " (458543, u'like'),\n",
        " (422581, u'3'),\n",
        " (421513, u'years')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Topic modeling\n",
      "\n",
      "Now we can do a topic model on the articles.  We'll use gensim LSI and distribute the work.<br>\n",
      "A few important things to note:<br>\n",
      "- The info for setting this up is here: http://radimrehurek.com/gensim/dist_lsi.html\n",
      "<br>\n",
      "\n",
      "To get it going we need to open a few terminal windows and set up the name server, workers, and distributor.  In this order run the following commands, the first three in the same terminal, and then each other in a new terminal, making one worker for every core you want to use:<br>\n",
      "- export PYRO_SERIALIZERS_ACCEPTED=pickle\n",
      "- export PYRO_SERIALIZER=pickle\n",
      "- \\$ python -m Pyro4.naming -n 0.0.0.0 &\n",
      "- \\$ python -m gensim.models.lsi_worker & ### note, run this one for each worker you would like to create\n",
      "- \\$ python -m gensim.models.lsi_dispatcher &"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_hundred = tokenized_articles.take(10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(first_hundred)\n",
      "corpus = [dictionary.doc2bow(text) for text in first_hundred]\n",
      "\n",
      "tfidf = models.TfidfModel(corpus) \n",
      "corpus_tfidf = tfidf[corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200, chunksize=10, distributed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(num_topics=2, num_words=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#LDA Example"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_topics = 50\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics, chunksize=2000, passes=10)#distributed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.print_topics(num_topics=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[u'0.002*Lightning + 0.002*Tampa + 0.001*Girardi + 0.001*Hagelin + 0.001*Lecavalier + 0.001*Tortorella + 0.001*tumors + 0.001*prom + 0.001*Stamkos + 0.001*Ledger',\n",
        " u'0.003*cent + 0.003*Snedeker + 0.002*earnings + 0.002*investment + 0.002*NASDAQ + 0.002*NYSE + 0.002*Pebble + 0.002*Mickelson + 0.002*currencies + 0.002*cash',\n",
        " u'0.003*Morsi + 0.001*palace + 0.001*emergencies + 0.001*Farmers + 0.001*dredging + 0.001*Herbalife + 0.001*unrest + 0.001*Postal + 0.001*harbors + 0.000*Salvation',\n",
        " u'0.002*mice + 0.002*genes + 0.001*drug + 0.001*obesity + 0.001*cancer + 0.001*disease + 0.001*SARS + 0.001*fat + 0.001*cubic + 0.001*downhill',\n",
        " u'0.003*iWatch + 0.002*glass + 0.002*Glass + 0.002*curved + 0.002*wearable + 0.002*smartwatch + 0.001*Apple + 0.001*HP + 0.001*Foxconn + 0.001*Willow',\n",
        " u'0.001*labour + 0.001*ultimate + 0.001*zombies + 0.001*Caribbean + 0.001*legislators + 0.001*pot + 0.001*plea + 0.001*Constitution + 0.001*Taking + 0.001*inform',\n",
        " u'0.001*Tierra + 0.001*GasBuddy + 0.001*Disney + 0.001*AshLee + 0.001*dipped + 0.001*Bills + 0.001*Radian + 0.001*EDT + 0.001*Gaza + 0.001*Volkswagen',\n",
        " u'0.005*Verlander + 0.002*Reimer + 0.001*Bryzgalov + 0.001*Hahn + 0.001*Pluto + 0.001*UM + 0.001*RIM + 0.001*moons + 0.001*outages + 0.001*Scrivens',\n",
        " u'0.002*Vodafone + 0.001*ICE + 0.001*thigh + 0.001*Sachs + 0.001*Sadler + 0.001*Goldman + 0.001*Rebels + 0.001*Sri + 0.001*workshop + 0.001*KSU',\n",
        " u'0.005*Carnival + 0.004*ship + 0.003*cruise + 0.003*Triumph + 0.003*Z10 + 0.002*passengers + 0.001*Zap2it + 0.001*Mexico + 0.001*Progreso + 0.001*Galveston',\n",
        " u'0.002*Experience + 0.002*Airways + 0.002*Whitney + 0.002*AMR + 0.001*JT + 0.001*merger + 0.001*shutout + 0.001*Toews + 0.001*Horton + 0.001*airline',\n",
        " u'0.003*Duke + 0.002*poll + 0.002*Ligety + 0.002*Hoosiers + 0.002*votes + 0.001*Louisville + 0.001*Syracuse + 0.001*_ + 0.001*Notre + 0.001*Dame',\n",
        " u'0.001*Owen + 0.001*accidents + 0.001*MOSCOW + 0.001*Shankar + 0.001*Brodeur + 0.001*Deadly + 0.001*pickup + 0.001*Ravi + 0.001*presenter + 0.001*Instrumental',\n",
        " u'0.004*percent + 0.004*market + 0.004*billion + 0.003*oil + 0.003*tax + 0.003*economy + 0.003*growth + 0.003*economic + 0.003*markets + 0.003*China',\n",
        " u'0.001*coyote + 0.001*Prediction + 0.001*Kill + 0.001*Johnston + 0.001*Paper + 0.001*feud + 0.001*sanity + 0.001*molecules + 0.001*coyotes + 0.001*Ramos',\n",
        " u'0.003*u0027s + 0.003*said + 0.002*people + 0.002*u0027t + 0.002*police + 0.002*Dorner + 0.002*Obama + 0.002*says + 0.002*would + 0.002*like',\n",
        " u'0.001*Sheen + 0.001*Jeter + 0.001*robbery + 0.001*rivers + 0.001*Revenge + 0.001*Kuroda + 0.001*Cumulus + 0.001*Wanted + 0.001*Arpaio + 0.001*kidnapping',\n",
        " u'0.005*iPhone + 0.003*Samsung + 0.003*Android + 0.003*HTC + 0.002*smartphone + 0.002*iPad + 0.002*Nexus + 0.002*Laden + 0.002*devices + 0.002*LG',\n",
        " u'0.003*u0026P + 0.003*tornado + 0.003*Nasdaq + 0.003*index + 0.003*Dell + 0.003*Bieber + 0.003*snow + 0.003*storm + 0.002*Mississippi + 0.002*Hattiesburg',\n",
        " u'0.002*Mahony + 0.002*Dew + 0.001*Kickstart + 0.001*juice + 0.001*mph + 0.001*drink + 0.001*Mountain + 0.001*archdiocese + 0.001*infants + 0.001*PepsiCo',\n",
        " u'0.007*stampede + 0.006*Allahabad + 0.004*Kumbh + 0.004*railway + 0.003*station + 0.003*festival + 0.003*Mela + 0.002*trains + 0.002*Railway + 0.002*pilgrims',\n",
        " u'0.012*Pope + 0.012*Benedict + 0.010*pope + 0.007*Vatican + 0.006*XVI + 0.004*church + 0.004*Cardinal + 0.004*Church + 0.004*Catholic + 0.004*resign',\n",
        " u'0.002*Kim + 0.002*Maggie + 0.002*Kardashian + 0.002*MGM + 0.001*burgers + 0.001*Darwin + 0.001*Kanye + 0.001*JB + 0.001*statue + 0.001*Rio',\n",
        " u'0.001*Heart + 0.001*Athens + 0.001*Teen + 0.001*Felton + 0.001*Inquirer + 0.001*adjustment + 0.001*Zeballos + 0.001*Berlusconi + 0.001*Month + 0.001*fat',\n",
        " u'0.003*Turkey + 0.003*Turkish + 0.002*Syrian + 0.002*Novo + 0.002*Pride + 0.002*dam + 0.002*border + 0.002*Nordisk + 0.001*Tresiba + 0.001*FDA',\n",
        " u'0.001*Brewer + 0.001*coronavirus + 0.001*Sars + 0.001*infected + 0.001*HPA + 0.001*mortality + 0.001*demo + 0.001*Dominican + 0.001*Rams + 0.001*Cunningham',\n",
        " u'0.007*Hagel + 0.006*meat + 0.005*horsemeat + 0.004*Panetta + 0.003*Findus + 0.003*nomination + 0.002*Senate + 0.002*Graham + 0.002*Gras + 0.002*Mardi',\n",
        " u'0.003*crocodile + 0.003*marijuana + 0.002*Lolong + 0.001*Snake + 0.001*abortion + 0.001*Bunawan + 0.001*Hawaii + 0.001*captivity + 0.001*Gates + 0.001*sands',\n",
        " u'0.009*Celtics + 0.005*Lakers + 0.004*Heat + 0.004*Rockets + 0.004*Clippers + 0.004*Garnett + 0.004*Pierce + 0.004*NBA + 0.003*Bobcats + 0.003*Kings',\n",
        " u'0.002*zombie + 0.002*Ericsson + 0.002*Wings + 0.001*repeal + 0.001*births + 0.001*Pluto + 0.001*moons + 0.001*traumatic + 0.001*Browns + 0.001*Simon',\n",
        " u'0.005*Surface + 0.004*Rick + 0.004*Daryl + 0.003*Windows + 0.002*Paterno + 0.002*Pro + 0.002*AMC + 0.002*Andrea + 0.002*Microsoft + 0.001*Freeh',\n",
        " u'0.001*Ricks + 0.001*Labour + 0.001*Cyrus + 0.001*acoustic + 0.001*medicines + 0.001*Unity + 0.000*kiwi + 0.000*Fairfield + 0.000*Boulder + 0.000*Miley',\n",
        " u'0.008*Guru + 0.006*Afzal + 0.005*Kashmir + 0.003*execution + 0.003*Financial + 0.003*Ltd + 0.003*Times + 0.003*hanging + 0.002*Jammu + 0.002*FT',\n",
        " u'0.001*Gaming + 0.001*Planned + 0.001*Parenthood + 0.001*Netanyahu + 0.001*u003cstrong + 0.001*Blackberry + 0.001*Oladipo + 0.001*Crean + 0.001*Zeller + 0.001*float',\n",
        " u'0.005*Grammy + 0.004*Apple + 0.003*Grammys + 0.003*album + 0.003*Awards + 0.003*song + 0.002*performance + 0.002*Ocean + 0.002*Best + 0.002*music',\n",
        " u'0.004*Mali + 0.004*Arias + 0.003*Malian + 0.002*Gao + 0.002*French + 0.002*Jodi + 0.002*Mailbox + 0.001*Illustrated + 0.001*troops + 0.001*soldiers',\n",
        " u'0.001*Abdel + 0.001*Langevin + 0.001*Rahman + 0.001*mythical + 0.001*Azhar + 0.001*obscene + 0.000*Mursi + 0.000*deduction + 0.000*Thinkin + 0.000*stuffed',\n",
        " u'0.006*Vick + 0.005*u003d + 0.004*Eagles + 0.004*u003e + 0.003*Spurs + 0.003*Leafs + 0.003*u003c + 0.002*Hurricanes + 0.002*quarterback + 0.002*Philadelphia',\n",
        " u'0.002*independence + 0.002*Scotland + 0.001*Springfield + 0.001*Scottish + 0.001*u0026T + 0.001*Woman + 0.001*referendum + 0.001*Tennessee + 0.001*circus + 0.001*myopia',\n",
        " u'0.006*beef + 0.002*Bruins + 0.002*Hello + 0.001*Carson + 0.001*drill + 0.001*Emanuel + 0.001*Morrissey + 0.001*Wonderland + 0.001*GAAP + 0.001*Akron',\n",
        " u'0.002*Erickson + 0.001*meals + 0.001*asteroids + 0.001*comet + 0.001*pit + 0.001*binoculars + 0.001*Utes + 0.001*sky + 0.001*metre + 0.001*BYU',\n",
        " u'0.004*game + 0.003*points + 0.003*season + 0.003*1 + 0.003*3 + 0.003*0 + 0.002*2 + 0.002*team + 0.002*million + 0.002*5',\n",
        " u'0.004*goal + 0.004*goals + 0.004*NHL + 0.003*Islanders + 0.003*Devils + 0.003*puck + 0.003*Blackhawks + 0.003*Penguins + 0.003*saves + 0.002*Rangers',\n",
        " u'0.004*mine + 0.004*nuclear + 0.004*Korea + 0.003*Syria + 0.003*coal + 0.003*Russia + 0.003*North + 0.003*Syrian + 0.002*Egypt + 0.002*opposition',\n",
        " u'0.005*asteroid + 0.004*Earth + 0.002*DA14 + 0.002*Grabovski + 0.002*NASA + 0.002*sky + 0.002*Pacioretty + 0.001*Flacco + 0.001*Canadiens + 0.001*diary',\n",
        " u'0.002*Thief + 0.002*Identity + 0.002*McCarthy + 0.001*Suicide + 0.001*Melissa + 0.001*Juventus + 0.001*Episode + 0.001*Bateman + 0.001*wsj + 0.001*Braun',\n",
        " u'0.002*Amin + 0.002*Salwa + 0.001*Buckwild + 0.001*Taro + 0.000*Vigneron + 0.000*succumbing + 0.000*Sacred + 0.000*Phillies + 0.000*laughter + 0.000*Couture',\n",
        " u'0.002*Colgate + 0.002*Shavack + 0.001*WBC + 0.001*Granger + 0.001*congregation + 0.001*Rivera + 0.001*Palmolive + 0.001*Believe + 0.001*WILMINGTON + 0.001*Nationals',\n",
        " u'0.001*Dorners + 0.001*Rescue + 0.001*Sixers + 0.001*Vision + 0.001*gatherings + 0.001*Diaw + 0.001*Haaretz + 0.001*Quinnipiac + 0.001*overcrowding + 0.001*Eye',\n",
        " u'0.001*bare + 0.001*problematic + 0.001*rover + 0.001*costumes + 0.001*wardrobe + 0.001*buttocks + 0.001*curves + 0.001*curvature + 0.001*buttock + 0.001*clothing']"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Appendix/Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(nltk.word_tokenize(\"John's big idea isn't all that bad.\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "[('John', 'NNP'),\n",
        " (\"'s\", 'POS'),\n",
        " ('big', 'JJ'),\n",
        " ('idea', 'NN'),\n",
        " ('is', 'VBZ'),\n",
        " (\"n't\", 'RB'),\n",
        " ('all', 'DT'),\n",
        " ('that', 'DT'),\n",
        " ('bad', 'JJ'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}