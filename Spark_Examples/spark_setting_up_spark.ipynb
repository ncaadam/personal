{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Setting Up Pyspark in iPython Notebook\n",
    "\n",
    "1) To set up pyspark, first download and unzip spark 1.3.0: https://spark.apache.org/downloads.html<br>\n",
    "2) If you are using Windows: don't; just get a linux VM<br>\n",
    "3) If you are using a mac or linux machine, you need to build Spark. Go to the terminal and navigate to the directory where you unzipped spark.  Then build spark with this command: <br>\n",
    "\n",
    "```\n",
    "$ sbt/sbt -Pyarn -Phadoop-2.3 assembly\n",
    "```\n",
    "\n",
    "4) Update your bash file to point to spark and add spark to the python path<br>\n",
    "```\n",
    "$ sudo nano ~/.bashrc\n",
    "```\n",
    "\n",
    "add the following lines: \n",
    "```\n",
    "export SPARK_HOME=<<location of your spark folder>> #e.g., /home/ai2/Documents/spark-1.3.0\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "```\n",
    "\n",
    "CTRL-X to save and exit, then:\n",
    "```\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "5) Navigate to your python folder and launch ipython notebook<br>\n",
    "```\n",
    "$ ipython notebook\n",
    "```\n",
    "\n",
    "6) Once you're in the notebook, use the setup below as an example for including spark in a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure Spark Settings\n",
    "conf=SparkConf()\n",
    "#conf.set(\"spark.executor.memory\", \"1g\")\n",
    "#conf.set(\"spark.cores.max\", \"2\")\n",
    "conf.setAppName(\"My App\")\n",
    "\n",
    "## Initialize SparkContext\n",
    "sc = SparkContext('local[*]', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7ff81772a3d0>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readme = sc.textFile('/home/ai2/Documents/spark-1.3.0/README.md') #edit location of file for your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/ai2/Documents/spark-1.3.0/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/ai2/Documents/spark-1.3.0/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#', u'Apache', u'Spark', u'Spark', u'is']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = readme.flatMap(lambda x: x.split())\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, u'the'),\n",
       " (14, u'Spark'),\n",
       " (14, u'to'),\n",
       " (11, u'for'),\n",
       " (10, u'and'),\n",
       " (9, u'a'),\n",
       " (8, u'##'),\n",
       " (7, u'run'),\n",
       " (6, u'is'),\n",
       " (6, u'on')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
