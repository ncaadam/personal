{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bson.objectid import ObjectId\n",
      "import copy \n",
      "import pymongo\n",
      "from pymongo import MongoClient\n",
      "\n",
      "from mongoengine import *\n",
      "import sys,datetime,uuid,random\n",
      "\n",
      "import logging\n",
      "import json\n",
      "logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "from gensim import corpora, models, similarities\n",
      "import gensim, bz2\n",
      "import numpy\n",
      "SOME_FIXED_SEED = 50  #for making the gensim topic results fixed \n",
      "numpy.random.seed(SOME_FIXED_SEED)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#mongoengine DB connection\n",
      "DB_NAME = 'mysite'\n",
      "connect(DB_NAME)\n",
      "\n",
      "#pymongo DB connection\n",
      "client = MongoClient()\n",
      "db = client.mysite"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursorall=db.all_user_tweets.find()\n",
      "def getMongoTweet(n):  #return the first n user's tweet data as a list of strings. each string is a user's 200 tweets added together\n",
      "\tj = 0\n",
      "\tdocuments = []\n",
      "\tindividual_twt = ''\n",
      "\tfor profile in cursorall:\t   \n",
      "\t   individual_twt = ''\n",
      "\t   j = j + 1\n",
      "\t   if j == n:\n",
      "\t      break\n",
      "      \n",
      "\t   for tweets in profile['tweets']:\n",
      "\t            individual_twt = individual_twt + tweets['text'].encode('ascii', 'ignore')       \n",
      "                \n",
      "\t   documents.append(individual_twt)\n",
      "\treturn documents\n",
      "\n",
      "documents = getMongoTweet(10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9999\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#removed common words(stopwords) and tokenized (http://www.ranks.nl/stopwords)\n",
      "stoplist = set('! x i c ;) jajaja jajajaja vos eu pene tu se da si wanna mi e ip lia amor til end\\' you: un\tuna\tunas\tunos\tuno\tsobre\ttodo tras\totro\talguno\talguna\talgunos\talgunas\tser\tes\tsoy\teres\tsomos\tsois\testoy\testa\testamos\testais\testan\tcomo\ten\tpara\tatras\tporque\tpor\testado\testaba\tante\tantes\tsiendo\tambos\tpero\tpor\tpoder\tpuede\tpuedo\tpodemos\tpodeis\tpueden\tfui\tfue\tfuimos\tfueron\thacer\thago\thace\thacemos\thaceis\thacen\tcada\tfin\tincluso\tprimero\tdesde\tconseguir\tconsigo\tconsigue\tconsigues\tconseguimos\tconsiguen\tir\tvoy\tva\tvamos\tvais\tvan\tvaya\tgueno\tha\ttener\ttengo\ttiene\ttenemos\tteneis\ttienen\tel\tla\tlo\tlas\tlos\tsu\taqui\tmio\ttuyo\tellos\tellas\tnos\tnosotros\tvosotros\tvosotras\tsi\tdentro\tsolo\tsolamente\tsaber\tsabes\tsabe\tsabemos\tsabeis\tsaben\tultimo\tlargo\tbastante\thaces\tmuchos\taquellos\taquellas\tsus\tentonces\ttiempo\tverdad\tverdadero\tverdadera\tcierto\tciertos\tcierta\tciertas\tintentar\tintento\tintenta\tintentas\tintentamos\tintentais\tintentan\tdos\tbajo\tarriba\tencima\tusar\tuso\tusas\tusa\tusamos\tusais\tusan\templear\templeo\templeas\templean\tampleamos\templeais\tvalor\tmuy\tera\teras\teramos\teran\tmodo\tbien\tcual\tcuando\tdonde\tmientras\tquien\tcon\tentre\tsin\ttrabajo\ttrabajar\ttrabajas\ttrabaja\ttrabajamos\ttrabajais\ttrabajan\tpodria\tpodrias\tpodriamos\tpodrian\tpodriais\tyo\taquelpor la el o en es sa mo jai plz | ; . y dm ha te na ang un ...rt lo ka fuck fucking que de follow bahahaha pp grin hoes bruh nom cc abt bae wld har shld d bb ur tt ooh w \\/ yep cos hv nigga niggas awhh bby ing r hoe ... lmao shit smh bitch smile lol u im lt rt gt hahah haha ^ via , : - ~ $ : a\tabout\tabove\tafter\tagain\tagainst\tall\tam\tan\tand\tany\tare\taren\\'t\tas\tat\tbe\tbecause\tbeen\tbefore\tbeing\tbelow\tbetween\tboth\tbut\tby\tcan\\'t\tcannot\tcould\tcouldn\\'t\tdid\tdidn\\'t\tdo\tdoes\tdoesn\\'t\tdoing\tdon\\'t\tdown\tduring\teach\tfew\tfor\tfrom\tfurther\thad\thadn\\'t\thas\thasn\\'t\thave\thaven\\'t\thaving\the\the\\'d\the\\'ll\the\\'s\ther\there\there\\'s\thers\therself\thim\thimself\this\thow\thow\\'s\ti\ti\\'d\ti\\'ll\ti\\'m\ti\\'ve\tif\tin\tinto\tis\tisn\\'t\tit\tit\\'s\tits\titself\tlet\\'s\tme\tmore\tmost\tmustn\\'t\tmy\tmyself\tno\tnor\tnot\tof\toff\ton\tonce\tonly\tor\tother\tought\tour\tours\tout\tover\town\tsame\tshan\\'t\tshe\tshe\\'d\tshe\\'ll\tshe\\'s\tshould\tshouldn\\'t\tso\tsome\tsuch\tthan\tthat\tthat\\'s\tthe\ttheir\ttheirs\tthem\tthemselves\tthen\tthere\tthere\\'s\tthese\tthey\tthey\\'d\tthey\\'ll\tthey\\'re\tthey\\'ve\tthis\tthose\tthrough\tto\ttoo\tunder\tuntil\tup\tvery\twas\twasn\\'t\twe\twe\\'d\twe\\'ll\twe\\'re\twe\\'ve\twere\tweren\\'t\twhat\twhat\\'s\twhen\twhen\\'s\twhere\twhere\\'s\twhich\twhile\twho\twho\\'s\twhom\twhy\twhy\\'s\twith\twon\\'t\twould\twouldn\\'t\tyou\tyou\\'d\tyou\\'ll\tyou\\'re\tyou\\'ve\tyour\tyours\tyourself\tyourselves'.split())\n",
      "\n",
      "texts = [[word for word in document.lower().split() #remove stopwords and tokenize words\n",
      "\t\t\tif word not in stoplist]        \n",
      "\t\t for document in documents]\n",
      "all_tokens = sum(texts, []) #all the tokenized words \n",
      "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1) #removed words that appear only one time \n",
      "texts = [[word for word in text if word not in tokens_once and '@' not in word and 'http' not in word]\n",
      "          for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeCorpus(dictionary, document):\n",
      "    stoplist = set('! x i c ;) jajaja jajajaja vos eu pene tu se da si wanna mi e ip lia amor til end\\' you: un\tuna\tunas\tunos\tuno\tsobre\ttodo tras\totro\talguno\talguna\talgunos\talgunas\tser\tes\tsoy\teres\tsomos\tsois\testoy\testa\testamos\testais\testan\tcomo\ten\tpara\tatras\tporque\tpor\testado\testaba\tante\tantes\tsiendo\tambos\tpero\tpor\tpoder\tpuede\tpuedo\tpodemos\tpodeis\tpueden\tfui\tfue\tfuimos\tfueron\thacer\thago\thace\thacemos\thaceis\thacen\tcada\tfin\tincluso\tprimero\tdesde\tconseguir\tconsigo\tconsigue\tconsigues\tconseguimos\tconsiguen\tir\tvoy\tva\tvamos\tvais\tvan\tvaya\tgueno\tha\ttener\ttengo\ttiene\ttenemos\tteneis\ttienen\tel\tla\tlo\tlas\tlos\tsu\taqui\tmio\ttuyo\tellos\tellas\tnos\tnosotros\tvosotros\tvosotras\tsi\tdentro\tsolo\tsolamente\tsaber\tsabes\tsabe\tsabemos\tsabeis\tsaben\tultimo\tlargo\tbastante\thaces\tmuchos\taquellos\taquellas\tsus\tentonces\ttiempo\tverdad\tverdadero\tverdadera\tcierto\tciertos\tcierta\tciertas\tintentar\tintento\tintenta\tintentas\tintentamos\tintentais\tintentan\tdos\tbajo\tarriba\tencima\tusar\tuso\tusas\tusa\tusamos\tusais\tusan\templear\templeo\templeas\templean\tampleamos\templeais\tvalor\tmuy\tera\teras\teramos\teran\tmodo\tbien\tcual\tcuando\tdonde\tmientras\tquien\tcon\tentre\tsin\ttrabajo\ttrabajar\ttrabajas\ttrabaja\ttrabajamos\ttrabajais\ttrabajan\tpodria\tpodrias\tpodriamos\tpodrian\tpodriais\tyo\taquelpor la el o en es sa mo jai plz | ; . y dm ha te na ang un ...rt lo ka fuck fucking que de follow bahahaha pp grin hoes bruh nom cc abt bae wld har shld d bb ur tt ooh w \\/ yep cos hv nigga niggas awhh bby ing r hoe ... lmao shit smh bitch smile lol u im lt rt gt hahah haha ^ via , : - ~ $ : a\tabout\tabove\tafter\tagain\tagainst\tall\tam\tan\tand\tany\tare\taren\\'t\tas\tat\tbe\tbecause\tbeen\tbefore\tbeing\tbelow\tbetween\tboth\tbut\tby\tcan\\'t\tcannot\tcould\tcouldn\\'t\tdid\tdidn\\'t\tdo\tdoes\tdoesn\\'t\tdoing\tdon\\'t\tdown\tduring\teach\tfew\tfor\tfrom\tfurther\thad\thadn\\'t\thas\thasn\\'t\thave\thaven\\'t\thaving\the\the\\'d\the\\'ll\the\\'s\ther\there\there\\'s\thers\therself\thim\thimself\this\thow\thow\\'s\ti\ti\\'d\ti\\'ll\ti\\'m\ti\\'ve\tif\tin\tinto\tis\tisn\\'t\tit\tit\\'s\tits\titself\tlet\\'s\tme\tmore\tmost\tmustn\\'t\tmy\tmyself\tno\tnor\tnot\tof\toff\ton\tonce\tonly\tor\tother\tought\tour\tours\tout\tover\town\tsame\tshan\\'t\tshe\tshe\\'d\tshe\\'ll\tshe\\'s\tshould\tshouldn\\'t\tso\tsome\tsuch\tthan\tthat\tthat\\'s\tthe\ttheir\ttheirs\tthem\tthemselves\tthen\tthere\tthere\\'s\tthese\tthey\tthey\\'d\tthey\\'ll\tthey\\'re\tthey\\'ve\tthis\tthose\tthrough\tto\ttoo\tunder\tuntil\tup\tvery\twas\twasn\\'t\twe\twe\\'d\twe\\'ll\twe\\'re\twe\\'ve\twere\tweren\\'t\twhat\twhat\\'s\twhen\twhen\\'s\twhere\twhere\\'s\twhich\twhile\twho\twho\\'s\twhom\twhy\twhy\\'s\twith\twon\\'t\twould\twouldn\\'t\tyou\tyou\\'d\tyou\\'ll\tyou\\'re\tyou\\'ve\tyour\tyours\tyourself\tyourselves'.split())\n",
      "\n",
      "    texts = [word for word in document.lower().split() #remove stopwords and tokenize words\n",
      "\t\t\tif word not in stoplist]        \n",
      "\t\t\n",
      "    all_tokens = sum(texts, [])\n",
      "    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1) #removed words that appear only one time \n",
      "    texts = [word for word in texts if word not in tokens_once and '@' not in word and 'http' not in word]\n",
      "    \n",
      "    corpus = dictionary.doc2bow(texts)  \n",
      "\n",
      "    return corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts] \n",
      "\n",
      "\n",
      "tfidf = models.TfidfModel(corpus) #initialize the tranformation model\n",
      "corpus_tfidf = tfidf[corpus] #corpus is list of term, ratio pairs in each document\n",
      "\n",
      "lda = gensim.models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=20, update_every=0, chunksize=10000, passes=1)\n",
      "\n",
      "alltopic = [] #this section saves the overall topics, their terms and ratios\n",
      "for i in range(0, 20):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    one_topic = []\n",
      "    for term in temp:\n",
      "        #terms.append(term[1])\n",
      "        one_topic_word = [term[1], term[0]]\n",
      "        one_topic.append(one_topic_word)\n",
      "    \n",
      "    alltopic.append(one_topic) \n",
      "    \n",
      "def getOneWordCloud(alltopic,one_user): #This function gets the whole topic dist, one user's dist and returns list of term/ratios for that user\n",
      "    #print one_user\n",
      "    one_user_cloud = []   \n",
      "    for one_topic in one_user: #one_topic is (3, 0.83495049520646281)\n",
      "         topic_dist = []\n",
      "         topic_dist = copy.deepcopy(alltopic[one_topic[0]])\n",
      "         for i in range(0,10):\n",
      "             #topic_dist is the first topic and its distribution this user has. \n",
      "             #print \"pre topic_dist\" + str(topic_dist)\n",
      "             topic_dist[i][1] = topic_dist[i][1]*one_topic[1]*100000\n",
      "             #print \"\"\n",
      "             #print \"post topic_dist\" + str(topic_dist)\n",
      "             #print \"\"\n",
      "         #print \"topic dist:   \"+str(topic_dist)\n",
      "         one_user_cloud = one_user_cloud + topic_dist\n",
      "    #print \"one_user_cloud:   \"+ str(one_user_cloud)\n",
      "        \n",
      "    return one_user_cloud\n",
      "\n",
      "\n",
      "# In[33]:\n",
      "\n",
      "def getAllDocTopicDist(corpus,lda):  #to get the topic distribution of each document in the whole document collection\n",
      "\n",
      "\tDocTopics = []\n",
      "\tfor cor in corpus:\n",
      "\t\tDocTopics.append(lda[cor])\n",
      "\treturn DocTopics\n",
      "\n",
      "#print lda.print_topic(2)\n",
      "#print lda.print_topic(3)\n",
      "every = getAllDocTopicDist(corpus,lda)\n",
      "\n",
      "\n",
      "# In[53]:\n",
      "\n",
      "\n",
      "alltopic = [] #this section saves the overall topics, their terms and ratios\n",
      "for i in range(0, 20):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    one_topic = []\n",
      "    for term in temp:\n",
      "        #terms.append(term[1])\n",
      "        one_topic_word = [term[1], term[0]]\n",
      "        one_topic.append(one_topic_word)\n",
      "    \n",
      "    alltopic.append(one_topic) \n",
      "\n",
      "\n",
      "\n",
      "#Creat mongodb collection for topic modeling result\n",
      "class Topic(Document): # TODO: TBD\n",
      "    topic_num = IntField()\n",
      "    \n",
      "def create_topic_result_object():\n",
      "    topic_result = Topic()\n",
      "    return topic_result\n",
      "\n",
      "\n",
      "#Save topic modeling result in MongoDB!!!!\n",
      "for i in range(0, 20):\n",
      "    topic_result =create_topic_result_object()\n",
      "    topic_result.topic_num  = i \n",
      "    topic_result.save()\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    temp = list(temp)   \n",
      "    db.topic.update({\"topic_num\":i},{\"$set\":{\"topic_ratio\":temp}})     \n",
      "    \n",
      "#documents = getMongoTweet(118)\n",
      "customer_cursorall=db.customer_data.find()\n",
      "documents = [] \n",
      "for profile in customer_cursorall:\n",
      "    #print \"customer objexct id:\" + str(profile['_id'])\n",
      "    s = profile['collected_data']['twitter_tweets'] #each customer's tweet objectid\n",
      "    twt_id = db.all_user_tweets.find_one({\"_id\":s}) #tweet object in all_user_tweets collection\n",
      "    individual_twt = ''\n",
      "    \n",
      "    for tweets in twt_id['tweets']:\n",
      "        #print twt_id\n",
      "        individual_twt = individual_twt + tweets['text'].encode('ascii', 'ignore')\n",
      "        #print \"individual twt is : \"+str(individual_twt)\n",
      "        #print \"\"\n",
      "\t            #i = i+1\n",
      "\t            #if i == 2: \n",
      "\t            #    break\n",
      "        \n",
      "    #documents.append(individual_twt)\n",
      "    print individual_twt\n",
      "    #individual_bow=makeCorpus(dictionary, individual_twt)\n",
      "    vec_bow = dictionary.doc2bow(individual_twt.lower().split()) \n",
      "    vec_lda = lda[vec_bow]\n",
      "    db.customer_data.update({\"_id\":profile['_id']},{\"$set\":{\"derived_data.tm_dist\":vec_lda}}) \n",
      "    db.customer_data.update({\"_id\":profile['_id']},{\"$set\":{\"derived_data.word_cloud\":getOneWordCloud(alltopic,vec_lda)}}) \n",
      "    #print getOneWordCloud(alltopic,vec_lda)\n",
      "\n",
      "\n",
      "# In[ ]:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}