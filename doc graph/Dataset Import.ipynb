{
 "metadata": {
  "name": "",
  "signature": "sha256:e4dcb9e0fd95d3a046ce5149018afbebf93db255a7bf5774a1e61e7ce4cd1d20"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2 as u\n",
      "import requests as r\n",
      "import graphlab as gl\n",
      "from pyspark import SparkConf,SparkContext\n",
      "\n",
      "# Configure Spark Settings\n",
      "conf=SparkConf()\n",
      "conf.set(\"spark.executor.memory\", \"1g\")\n",
      "conf.set(\"spark.cores.max\", \"4\")\n",
      "conf.setAppName(\"My App\")\n",
      "\n",
      "## Initialize SparkContext\n",
      "sc = SparkContext('local[*]', conf=conf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[INFO] Start server at: ipc:///tmp/graphlab_server-20583 - Server binary: /Users/hdwall002/anaconda/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1423247373.log\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[INFO] GraphLab Server Version: 1.2.1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = u.urlopen('http://www.bloomapi.com/api/search/npi?key1=npi&op1=eq&value1=1346488400')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'u' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-3c118565a299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://www.bloomapi.com/api/search/npi?key1=npi&op1=eq&value1=1346488400'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'u' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test2 = r.get('http://www.bloomapi.com/api/search/npi?key1=npi&op1=eq&value1=1346488400')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test2.json()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#gl.SFrame('../../../data/docgraph/NPPES_Data_Dissemination_January_2015/npidata_20050523-20150111.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = sc.textFile('../../../data/docgraph/NPPES_Data_Dissemination_January_2015/npidata_20050523-20150111.csv').map(lambda x: x.split(','))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#header = test.first()\n",
      "#npidata = test.filter(lambda line: line != header)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "pyspark.rdd.PipelinedRDD"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(line):\n",
      "    new_line=[]\n",
      "    for i in line:\n",
      "        i=i.replace('\\\"','')\n",
      "        new_line.append(i)\n",
      "    return new_line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test2 = test.filter(lambda line: clean(line))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npidata_df = gl.SFrame.from_rdd(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o107.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 8.0 failed 1 times, most recent failure: Lost task 11.0 in stage 8.0 (TID 506, localhost): java.lang.Exception: Subprocess exited with status 134\n        org.apache.spark.rdd.PipedRDD$$anon$1.hasNext(PipedRDD.scala:162)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        org.apache.spark.rdd.PipedRDD$$anon$1.foreach(PipedRDD.scala:154)\n        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n        org.apache.spark.rdd.PipedRDD$$anon$1.to(PipedRDD.scala:154)\n        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n        org.apache.spark.rdd.PipedRDD$$anon$1.toBuffer(PipedRDD.scala:154)\n        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n        org.apache.spark.rdd.PipedRDD$$anon$1.toArray(PipedRDD.scala:154)\n        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)\n        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        java.lang.Thread.run(Thread.java:745)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-891565c09c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnpidata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/hdwall002/anaconda/lib/python2.7/site-packages/graphlab/data_structures/sframe.pyc\u001b[0m in \u001b[0;36mfrom_rdd\u001b[0;34m(cls, rdd)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# We get the location of an SFrame index file per Spark partition in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;31m# the result.  We assume that this is in partition order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mout_sf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/hdwall002/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/hdwall002/anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o107.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 8.0 failed 1 times, most recent failure: Lost task 11.0 in stage 8.0 (TID 506, localhost): java.lang.Exception: Subprocess exited with status 134\n        org.apache.spark.rdd.PipedRDD$$anon$1.hasNext(PipedRDD.scala:162)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        org.apache.spark.rdd.PipedRDD$$anon$1.foreach(PipedRDD.scala:154)\n        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n        org.apache.spark.rdd.PipedRDD$$anon$1.to(PipedRDD.scala:154)\n        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n        org.apache.spark.rdd.PipedRDD$$anon$1.toBuffer(PipedRDD.scala:154)\n        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n        org.apache.spark.rdd.PipedRDD$$anon$1.toArray(PipedRDD.scala:154)\n        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)\n        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:774)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        java.lang.Thread.run(Thread.java:745)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npidata_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
        "    <tr>\n",
        "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">X1</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1679576722\",\"1\",\"\",\"\",\"\"<br>,\"WIEBE\",\"DAVID\",\"A\", ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1588667638\",\"1\",\"\",\"\",\"\"<br>,\"PILCHER\",\"WILLIAM\", ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1497758544\",\"2\",\"\",\"&lt;UNA<br>VAIL&gt;\",\"CUMBERLAND CO ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1306849450\",\"1\",\"\",\"\",\"\"<br>,\"SMITSON\",\"HAROLD\",\" ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1215930367\",\"1\",\"\",\"\",\"\"<br>,\"GRESSOT\",\"LAURENT\", ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1023011178\",\"2\",\"\",\"&lt;UNA<br>VAIL&gt;\",\"NAPA VALLEY ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1932102084\",\"1\",\"\",\"\",\"\"<br>,\"ADUSUMILLI\",\"RAVI\", ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1841293990\",\"1\",\"\",\"\",\"\"<br>,\"WORTSMAN\",\"SUSAN ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1750384806\",\"1\",\"\",\"\",\"\"<br>,\"BISBEE\",\"ROBERT\",\"\" ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"1669475711\",\"1\",\"\",\"\",\"\"<br>,\"SUNG\",\"BIN\",\"SHENG\" ...</td>\n",
        "    </tr>\n",
        "</table>\n",
        "[10 rows x 1 columns]<br/>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "Columns:\n",
        "\tX1\tstr\n",
        "\n",
        "Rows: 10\n",
        "\n",
        "Data:\n",
        "+--------------------------------+\n",
        "|               X1               |\n",
        "+--------------------------------+\n",
        "| \"1679576722\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1588667638\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1497758544\",\"2\",\"\",\"<UNAV ... |\n",
        "| \"1306849450\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1215930367\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1023011178\",\"2\",\"\",\"<UNAV ... |\n",
        "| \"1932102084\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1841293990\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1750384806\",\"1\",\"\",\"\",\"\", ... |\n",
        "| \"1669475711\",\"1\",\"\",\"\",\"\", ... |\n",
        "+--------------------------------+\n",
        "[10 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}