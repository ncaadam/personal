{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a package of ipython notebooks intended to recreate the neo4j database for the DocGraph API (located [here](https://github.com/Analytics-Innovation-Incubator/Cohort-Builder)). There are datasets and python packages required to run these notebooks. See below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructructions\n",
    "1. Download datasets and extract appropriate files to the _'data'_ folder\n",
    "2. Install Python packages required _(if necessary)_\n",
    "3. Install neo4j _(if necessary)_\n",
    "4. Execute each ipython notebook in numericalorder (1-5)\n",
    "5. Compile graph.db from files generated (see notebook #6 for instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Required\n",
    "</p>\n",
    "1. Download each dataset below (they're on PwC google drive, so use that authentication)\n",
    "2. Extract each of the zip files\n",
    "3. Take the file with the name indicated in the sub-bullet and save it to the <i>'data'</i> folder in this directory\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>DocGraph Edge Data</b> - https://drive.google.com/open?id=0B15A78kQcodoaDFGWUJZc2ZjdDQ&authuser=0\n",
    "    - <i>DocGraph-2012-2013-Days365.csv</i>\n",
    "- <b>DocGraph Node Data</b> - https://drive.google.com/open?id=0B15A78kQcododGU4MlVSTmtLdzQ&authuser=0\n",
    "    - <i>DocGraph_Procedure.csv</i>\n",
    "- <b>NPPES Data</b> - https://drive.google.com/open?id=0B5VudqrtA4nUdDduQ3hnSVhUTnM&authuser=0\n",
    "    - <i>npidata_20050523-20150208.csv</i>\n",
    "- <b>HCPCS Description Data</b> - https://drive.google.com/open?id=0B5VudqrtA4nUb01xUXNZcmpva2c&authuser=0\n",
    "    - <i>Medicare-Physician-and-Other-Supplier-PUF-CY2012.txt</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Packages Required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graphlab-create 1.3 or higher\n",
    "    - https://github.com/dato-code/Dato-Core\n",
    "- pyspark 1.3 or higher (Apache-Spark)\n",
    "    - https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neo4j Installation Required\n",
    "- Install neo4j 2.2.0 or higher (community edition)\n",
    "    - http://neo4j.com/download/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These notebooks use graphlab to convert Spark RDDs to graphlab SFrames. To enable this capability, you must change your <b>Spark</b> configuration file to point Spark to a graphlab JAR file. See the <i>'Spark RDD -> Graphlab Tutorial'</i> notebook in this directory for an example of how to set it up.\n",
    "- When using Spark in these notebooks, feel free to change the configuration settings when initializing the SparkContext. <b>All of the default settings are 10 GB of memory and 1 core.</b> Example below...\n",
    "    - <code>conf.set(\"spark.executor.memory\", \"10g\")\n",
    "    - <code>conf.set(\"spark.cores.max\", \"1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
