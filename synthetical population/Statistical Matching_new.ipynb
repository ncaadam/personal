{
 "metadata": {
  "name": "",
  "signature": "sha256:9221c542146ea83ce853172f18e5d116b768dcffdaf5a262552f7172f835c953"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import pandas\n",
      "import pandas as pd\n",
      "\n",
      "#import distance for matching algo\n",
      "import scipy.spatial.distance as distance\n",
      "\n",
      "#import the Python3 division functionality\n",
      "from __future__ import division\n",
      "\n",
      "#import sklearn packages for training & testing models\n",
      "from sklearn.svm import SVC, SVR\n",
      "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import accuracy_score,mean_squared_error\n",
      "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "\n",
      "#import the datetime package\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def matching_columns(data,dataset_abbreviation,column_new_names_dict):\n",
      "    \"\"\"\n",
      "    Takes a full dataset, drops all columns but the ones identified in 'column_new_names_dict', \n",
      "    and renames the columns using the values in 'column_new_names_dict'. It also appends the abbreviation to\n",
      "    the front of each column name. \n",
      "    \n",
      "    Returns a pandas dataframe\n",
      "    \n",
      "    data - pandas data frame\n",
      "    dataset_abbreviation - an abbreviation to be used throughout the script (String)\n",
      "    column_new_names_dict - a dictionary with the original column names as keys (string) and the values as what you'd like to rename the column as (string)\n",
      "    \"\"\"\n",
      "    if isinstance(column_new_names_dict, dict) and isinstance(dataset_abbreviation, str):\n",
      "        subset = data[column_new_names_dict.keys()]\n",
      "        for key,value in column_new_names_dict.iteritems():\n",
      "            if column_new_names_dict[key][:len(dataset_abbreviation)] != dataset_abbreviation:\n",
      "                column_new_names_dict[key] = str(dataset_abbreviation) + '_' + value\n",
      "            else:\n",
      "                #print \"DATASET ALREADY NAMED\"\n",
      "                continue\n",
      "        subset_renamed = subset.rename(columns = column_new_names_dict)\n",
      "        return subset_renamed\n",
      "    else:\n",
      "        print \"PROBLEM\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def import_data(path,sheet_no = 0, row_skip = 0):\n",
      "    \"\"\"\n",
      "    Imports a dataset from the path variable and exports it as a pandas dataframe\n",
      "    \"\"\"\n",
      "    extension = path[path.rfind('.')+1:]\n",
      "    if extension == 'xlsx' or extension == 'xls':\n",
      "        #bring in your data\n",
      "        excel = pd.ExcelFile(path)\n",
      "        #parse the data into a pandas dataframe - parse accepts arguments for sheet name/number and rows to skip (for example)\n",
      "        data = excel.parse(sheetname=sheet_no,skiprows=row_skip,)\n",
      "        return data\n",
      "    elif extension == 'csv':\n",
      "        csv = pd.read_csv(path)\n",
      "        return csv\n",
      "    else:\n",
      "        return \"PROBLEM\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_distances(xa,xb,method):\n",
      "    \"\"\"\n",
      "    xa - 1-D array\n",
      "    xb - 2-D array\n",
      "    method - metric used to find the distance between each pair of the dataset\n",
      "             options for this argument are - 'braycurtis\u2019,\u2018canberra\u2019,\u2018chebyshev\u2019,\u2018cityblock\u2019,\u2018correlation\u2019,\u2018cosine\u2019,\u2018dice\u2019,\u2018euclidean\u2019,\u2018hamming\u2019,\u2018jaccard\u2019,\u2018kulsinski\u2019,\u2018mahalanobis\u2019,\u2018matching\u2019,\u2018minkowski\u2019,\u2018rogerstanimoto\u2019,\u2018russellrao\u2019,\u2018seuclidean\u2019,\u2018sokalmichener\u2019,\u2018sokalsneath\u2019,\u2018sqeuclidean\u2019,\u2018wminkowski\u2019,\u2018yule\u2019\n",
      "    \"\"\"\n",
      "    possibilities = [\"braycurtis\",\"canberra\",\"chebyshev\",\"cityblock\",\"correlation\",\"cosine\",\"dice\",\"euclidean\",\"hamming\",\"jaccard\",\"kulsinski\",\"mahalanobis\",\"matching\",\"minkowski\",\"rogerstanimoto\",\"russellrao\",\"seuclidean\",\"sokalmichener\",\"sokalsneath\",\"sqeuclidean\",\"wminkowski\",\"yule\"]\n",
      "    if method in possibilities:\n",
      "        try:\n",
      "            distances = distance.cdist(xa,xb,metric = method)\n",
      "        except:\n",
      "            distances = []\n",
      "        finally:\n",
      "            return distances"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_accuracy(data, xa_column_name, xb_column_name):\n",
      "    \"\"\"\n",
      "    Checks the accuracy of two columns in the same dataframe and returns the percent that are the same.\n",
      "    \n",
      "    data - pandas dataframe\n",
      "    xa_column_name - first column to be compared\n",
      "    xb_column_name - second column to be compared\n",
      "    \"\"\"\n",
      "    num_of_same = len(data[data[xa_column_name] == data[xb_column_name]])\n",
      "    num_total = len(data)\n",
      "    output = (num_of_same/num_total)*100\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def drop_and_split(data,column_to_drop = \"\"):\n",
      "    \"\"\"\n",
      "    Drops the column indicated in the 'column_to_drop' parameter and splits the data set into train and test datasets.\n",
      "    \n",
      "    Uses 20% of the data for the test dataset\n",
      "    \n",
      "    Returns train,test as pandas dataframes\n",
      "    \n",
      "    data - pandas dataframe\n",
      "    column_to_drop - the name of the column to drop from the dataset(string)\n",
      "    \"\"\"\n",
      "    #one_drop = data.drop(column_to_drop,1)\n",
      "    #column_names = list(one_drop.columns.values)\n",
      "    train, test = train_test_split(data, test_size=0.20, random_state = 24)\n",
      "    train = pd.DataFrame(train,columns=data.columns.values.tolist())\n",
      "    test = pd.DataFrame(test,columns=data.columns.values.tolist())\n",
      "    return train,test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(input_file, p_type, client_abbreviation, y_name):\n",
      "    acc_dict = {}\n",
      "    \n",
      "    train, test = drop_and_split(input_file)\n",
      "    \n",
      "    x_vars = []\n",
      "    y_var = \"\"\n",
      "    z_var = \"\"\n",
      "    \n",
      "    for col in train.columns.values:\n",
      "        if col.find(y_name) < 0 and col.find(\"STATE\") < 0:\n",
      "            x_vars.append(col)\n",
      "        elif col.find(y_name) > -1 and col.find(client_abbreviation) > -1:\n",
      "            y_var = col\n",
      "        elif col.find(y_name) > -1 and col.find(client_abbreviation) < 0:\n",
      "            z_var = col\n",
      "    \n",
      "    x = train[x_vars]\n",
      "    y = train[y_var]\n",
      "    test_x = test[x_vars]\n",
      "   \n",
      "    if(p_type == 'c'):\n",
      "        \n",
      "        #x = train[['SBI_AGE','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_AGE','PUMD_SALARY']]\n",
      "        #y = train['PUMD_NEW_HOME_STATUS']\n",
      "        #test_x = test[['SBI_AGE','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_AGE','PUMD_SALARY']]\n",
      "    \n",
      "        #Matching Accuracy\n",
      "        #acc_dict['OVERALL'] = check_accuracy(input_file,'SBI_HOME_STATUS','PUMD_NEW_HOME_STATUS')\n",
      "        acc_dict['OVERALL'] = accuracy_score(list(input_file[y_var]),list(input_file[z_var]))*100\n",
      "        \n",
      "        #SVM\n",
      "        clf = SVC()\n",
      "        clf.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_SVM'\n",
      "        test[new_col_name] = clf.predict(test_x)\n",
      "        acc_dict['SVM']= accuracy_score(list(test[y_var]),list(test[new_col_name]))*100\n",
      "        \n",
      "        #KNN\n",
      "        clf2 = KNeighborsClassifier()\n",
      "        clf2.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_KNN'\n",
      "        test[new_col_name] = clf2.predict(test_x)\n",
      "        #drop_acc = check_accuracy(test,'SBI_HOME_STATUS','HOME_STATUS_PREDICT_KNN')\n",
      "        #client_acc = check_accuracy(test,'PUMD_NEW_HOME_STATUS','HOME_STATUS_PREDICT_KNN')\n",
      "        #acc_dict['KNN'] = [drop_acc,client_acc]\n",
      "        acc_dict['KNN'] = accuracy_score(list(test[y_var]),list(test[new_col_name]))*100\n",
      "        \n",
      "        #Log Reg\n",
      "        clf3 = LogisticRegression()\n",
      "        clf3.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_LOG'\n",
      "        test[new_col_name] = clf3.predict(test_x)\n",
      "        acc_dict['LOG'] = accuracy_score(list(test[y_var]),list(test[new_col_name]))*100\n",
      "        \n",
      "        #Decision Tree\n",
      "        clf4 = DecisionTreeClassifier()\n",
      "        clf4.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_DT'\n",
      "        test[new_col_name] = clf4.predict(test_x)\n",
      "        acc_dict['DT'] = accuracy_score(list(test[y_var]),list(test[new_col_name]))*100\n",
      "        \n",
      "        return acc_dict\n",
      "        \n",
      "    elif(p_type == 'r'):\n",
      "        \n",
      "        #x = train[['SBI_HOME_STATUS','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_SALARY', 'PUMD_NEW_HOME_STATUS']]\n",
      "        #y = train['PUMD_AGE']\n",
      "        #test_x = test[['SBI_HOME_STATUS','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_SALARY','PUMD_NEW_HOME_STATUS']]\n",
      "        \n",
      "        #Overall\n",
      "        acc_dict['OVERALL'] = mean_squared_error(list(input_file[y_var]),list(input_file[z_var]))\n",
      "        \n",
      "        #SVM\n",
      "        clf = SVR()\n",
      "        clf.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_SVM'\n",
      "        test[new_col_name] = clf.predict(test_x)\n",
      "        acc_dict['SVM'] = mean_squared_error(list(test[y_var]),list(test[new_col_name]))\n",
      "        \n",
      "        #KNN\n",
      "        clf2 = KNeighborsRegressor()\n",
      "        clf2.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_KNN'\n",
      "        test[new_col_name] = clf2.predict(test_x)\n",
      "        acc_dict['KNN'] = mean_squared_error(list(test[y_var]),list(test[new_col_name]))\n",
      "        \n",
      "        #LIN\n",
      "        clf3 = LinearRegression(fit_intercept=False)\n",
      "        clf3.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_LIN'\n",
      "        test[new_col_name] = clf3.predict(test_x)\n",
      "        acc_dict['LIN'] = mean_squared_error(list(test[y_var]),list(test[new_col_name]))\n",
      "            \n",
      "        #Decision Tree    \n",
      "        clf4 = DecisionTreeRegressor()\n",
      "        clf4.fit(x,y)\n",
      "        new_col_name = y_name + '_PREDICT_DT'\n",
      "        test[new_col_name] = clf4.predict(test_x)\n",
      "        acc_dict['DT'] = mean_squared_error(list(test[y_var]),list(test[new_col_name]))\n",
      "        \n",
      "        return acc_dict\n",
      "        \n",
      "    else:\n",
      "        print \"the prediction type must be a regression or a classifier\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import the client data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Change the client abbreviation variable to an identifiable abbreviation you'd like to add to the front of your column names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#client abbreviation that will be used throughout the script\n",
      "client_abbreviation = 'PUMD'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Select the columns you'd like to match in the key of the dictionary and rename it in the value of the dictionary. (e.g. {'random_variable_name':'meaningful_variable_name'})\n",
      "\n",
      "Do not worry about adding dataset indicators in the variable names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#identify the columns we're interested in - change the varaible names here\n",
      "client_columns = {'NEWID':'ID','SEX_REF':'SEX','AGE_REF':'AGE','FSALARYX':'SALARY','STATE':'STATE_CODE',\n",
      "                  'CUTENURE':'ORIG_HOUSE_STATUS'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import the client dataset - if the dataset is large, this may take minutes to finish\n",
      "client_data = import_data(\"../../../data/synthetic_population/Interview_2013_FMLI_all.xlsx\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#grab the columns we're interested in and change the column names to more meaningful descriptors\n",
      "client_filtered = matching_columns(client_data,client_abbreviation,client_columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Transform the client data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#map values to different values to be consistent across datasets\n",
      "client_filtered[client_abbreviation + '_' + 'NEW_HOME_STATUS'] = client_filtered[client_abbreviation + '_' + 'ORIG_HOUSE_STATUS'].map({1:1,2:1,3:1,4:2,5:3,6:3})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client_filtered.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PUMD_SEX</th>\n",
        "      <th>PUMD_SALARY</th>\n",
        "      <th>PUMD_STATE_CODE</th>\n",
        "      <th>PUMD_ID</th>\n",
        "      <th>PUMD_AGE</th>\n",
        "      <th>PUMD_ORIG_HOUSE_STATUS</th>\n",
        "      <th>PUMD_NEW_HOME_STATUS</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2</td>\n",
        "      <td>      0</td>\n",
        "      <td>  2</td>\n",
        "      <td> 2545235</td>\n",
        "      <td> 82</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td>      0</td>\n",
        "      <td> 47</td>\n",
        "      <td> 2545245</td>\n",
        "      <td> 69</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td>  11000</td>\n",
        "      <td> 51</td>\n",
        "      <td> 2545255</td>\n",
        "      <td> 22</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 117000</td>\n",
        "      <td> 51</td>\n",
        "      <td> 2545275</td>\n",
        "      <td> 57</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2</td>\n",
        "      <td> 115000</td>\n",
        "      <td> 24</td>\n",
        "      <td> 2545305</td>\n",
        "      <td> 45</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "   PUMD_SEX  PUMD_SALARY  PUMD_STATE_CODE  PUMD_ID  PUMD_AGE  \\\n",
        "0         2            0                2  2545235        82   \n",
        "1         1            0               47  2545245        69   \n",
        "2         1        11000               51  2545255        22   \n",
        "3         1       117000               51  2545275        57   \n",
        "4         2       115000               24  2545305        45   \n",
        "\n",
        "   PUMD_ORIG_HOUSE_STATUS  PUMD_NEW_HOME_STATUS  \n",
        "0                       1                     1  \n",
        "1                       1                     1  \n",
        "2                       2                     1  \n",
        "3                       1                     1  \n",
        "4                       1                     1  "
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "state_codes = import_data('../data/State Codes.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create a dictionary with this structure {'State Code':'State Abbreviation'}\n",
      "state_code_dict = dict(zip(state_codes.Code, state_codes.Abb))\n",
      "\n",
      "#transform the state codes into state abbreviations based on our state code data\n",
      "client_filtered[client_abbreviation + '_' + 'STATE_ABB'] = client_filtered[client_abbreviation + '_' + 'STATE_CODE'].map(state_code_dict)\n",
      "\n",
      "#drop the original columns that were transformed\n",
      "client_final = client_filtered.drop([client_abbreviation + '_' + 'STATE_CODE',\n",
      "                                     client_abbreviation + '_' + 'ORIG_HOUSE_STATUS'],1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import the donor data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#donor abbreviation that will be used throughout the script\n",
      "donor_abbreviation = \"SBI\"\n",
      "\n",
      "#identify the columns we're interested in - change the varaible names here\n",
      "donor_columns = {'ID':'ID','OCALC_4':'AGE','OCALC_27':'SALARY','OCALC_10':'SEX','STATE':'STATE','G4':'HOME_STATUS'}\n",
      "\n",
      "#pull in donor dataset\n",
      "donor_data = import_data('../../../data/synthetic_population/SBI_Complete.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filter for the columns that we're interested in - this also renameds the columns\n",
      "donor_filtered = matching_columns(donor_data,donor_abbreviation,donor_columns)\n",
      "donor_final = donor_filtered"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create distance function algorithm using Mahalanobis distance: d = (xa - xb)'*CovXX^-1*(xa-xb)\n",
      "#choose number of AXA records to match and pull an array of that many records at random\n",
      "#import random\n",
      "#n = 20000\n",
      "#AXA_Sample = axa_data_final.ix[random.sample(axa_data_final.index,n)]\n",
      "#AXA_Sample.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create matrix of distance measurements\n",
      "XA = client_final[['PUMD_AGE','PUMD_SEX','PUMD_SALARY','PUMD_NEW_HOME_STATUS']]#,'PUMD_STATE_ABB']]\n",
      "XB = donor_final[['SBI_AGE','SBI_SEX','SBI_SALARY','SBI_HOME_STATUS']]#,'SBI_STATE']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create a dictionary with the statistical matching method as the key and the matching array as the value\n",
      "stat_matching_dict = {}\n",
      "potential_methods = [\"braycurtis\",\"canberra\",\"cityblock\",\"correlation\",\"cosine\",\"euclidean\",\"minkowski\",\"sqeuclidean\"]\n",
      "for method in potential_methods:\n",
      "    output = get_distances(XA,XB,method)\n",
      "    stat_matching_dict[method] = output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_input_file(method, stat_matching_dict,donor_data, donor_abbreviation, donor_columns, client_final, client_abbreviation):\n",
      "    #convert distances to a dataframe and add IDs as indices\n",
      "    distancesDF = pd.DataFrame(stat_matching_dict, index=pd.np.array(client_final[[client_abbreviation + '_ID']][client_abbreviation + '_ID']))\n",
      "    distancesDF.columns = pd.np.array(donor_final[[donor_abbreviation + '_ID']][donor_abbreviation + '_ID'])\n",
      "\n",
      "    matched_records = distancesDF.idxmin(axis=1).to_dict()\n",
      "    print method + \": IDs indexed...\" + str(datetime.now())\n",
      "    \n",
      "    donor_input = matching_columns(donor_data,donor_abbreviation,donor_columns)\n",
      "    donor_input.describe()\n",
      "    print method + \": Donor Input created...\" + str(datetime.now())\n",
      "    \n",
      "    #assemble input file fields from client and donor datasets based on matched IDs\n",
      "    input_file_list = []\n",
      "\n",
      "    for client_id, donor_id in matched_records.iteritems():\n",
      "        matched_record = donor_input[donor_input[donor_abbreviation + '_ID'] == donor_id]\n",
      "        matched_record[client_abbreviation + '_ID'] = client_id\n",
      "        input_file_list.append(matched_record)\n",
      "        \n",
      "    print method + \": matched records found and created...\" + str(datetime.now())\n",
      "\n",
      "    input_file = pd.concat(input_file_list)\n",
      "    print method + \": input file concatenated...\" + str(datetime.now())\n",
      "\n",
      "    #merge the donor data with the client data for comparison\n",
      "    input_file = pd.merge(input_file, client_final, how = 'left', left_on=client_abbreviation + '_ID', right_on=client_abbreviation + '_ID')\n",
      "    print method + \": input file merged...\" + str(datetime.now())\n",
      "    \n",
      "    input_file.to_csv(\"../data/outputs/\" + method + \".csv\")\n",
      "    print method + \": input file saved...\" + str(datetime.now())\n",
      "    print method, \"is complete\", str(datetime.now())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "potential_methods = [\"correlation\",\"cosine\",\"euclidean\",\"minkowski\",\"sqeuclidean\"]\n",
      "for method in potential_methods:\n",
      "    create_input_file(method,stat_matching_dict[method],donor_data, donor_abbreviation, donor_columns, client_final, client_abbreviation)\n",
      "    print \"Method - \" + method, \"complete\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "correlation: IDs indexed...2015-01-07 11:38:32.205608\n",
        "correlation: Donor Input created...2015-01-07 11:38:32.220459\n",
        "correlation: matched records found and created...2015-01-07 12:10:10.875844"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correlation: input file concatenated...2015-01-07 12:10:19.791789"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correlation: input file merged...2015-01-07 12:10:19.823948\n",
        "correlation: input file saved...2015-01-07 12:10:19.874586"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correlation is complete 2015-01-07 12:10:19.874904\n",
        "Method - correlation"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete\n",
        "cosine: IDs indexed...2015-01-07 12:10:20.517631"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cosine: Donor Input created...2015-01-07 12:10:20.526784\n",
        "cosine: matched records found and created...2015-01-07 12:43:33.214533"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cosine: input file concatenated...2015-01-07 12:43:42.150150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cosine: input file merged...2015-01-07 12:43:42.159625\n",
        "cosine: input file saved...2015-01-07 12:43:42.210907"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cosine is complete 2015-01-07 12:43:42.211215\n",
        "Method - cosine"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete\n",
        "euclidean: IDs indexed...2015-01-07 12:43:42.851633"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "euclidean: Donor Input created...2015-01-07 12:43:42.860801\n",
        "euclidean: matched records found and created...2015-01-07 13:16:49.861890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "euclidean: input file concatenated...2015-01-07 13:16:58.740613"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "euclidean: input file merged...2015-01-07 13:16:58.749877\n",
        "euclidean: input file saved...2015-01-07 13:16:58.801872"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "euclidean is complete 2015-01-07 13:16:58.802176\n",
        "Method - euclidean"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete\n",
        "minkowski: IDs indexed...2015-01-07 13:16:59.446645"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "minkowski: Donor Input created...2015-01-07 13:16:59.455738\n",
        "minkowski: matched records found and created...2015-01-07 13:50:15.956692"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "minkowski: input file concatenated...2015-01-07 13:50:24.942097"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "minkowski: input file merged...2015-01-07 13:50:24.951475\n",
        "minkowski: input file saved...2015-01-07 13:50:25.004329"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "minkowski is complete 2015-01-07 13:50:25.004646\n",
        "Method - minkowski"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete\n",
        "sqeuclidean: IDs indexed...2015-01-07 13:50:25.657065"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sqeuclidean: Donor Input created...2015-01-07 13:50:25.666224\n",
        "sqeuclidean: matched records found and created...2015-01-07 14:23:52.928185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sqeuclidean: input file concatenated...2015-01-07 14:24:01.927294"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sqeuclidean: input file merged...2015-01-07 14:24:01.936753\n",
        "sqeuclidean: input file saved...2015-01-07 14:24:01.988665"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sqeuclidean is complete 2015-01-07 14:24:01.988976\n",
        "Method - sqeuclidean"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:18: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#select statistical matching method to use\n",
      "method = 'cityblock'\n",
      "\n",
      "#convert distances to a dataframe and add IDs as indices\n",
      "distancesDF = pd.DataFrame(stat_matching_dict[method], index=pd.np.array(client_final[[client_abbreviation + '_ID']][client_abbreviation + '_ID']))\n",
      "distancesDF.columns = pd.np.array(donor_final[[donor_abbreviation + '_ID']][donor_abbreviation + '_ID'])\n",
      "\n",
      "matched_records = distancesDF.idxmin(axis=1).to_dict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "donor_input = matching_columns(donor_data,donor_abbreviation,donor_columns)\n",
      "donor_input.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DATASET ALREADY NAMED\n",
        "DATASET ALREADY NAMED\n",
        "DATASET ALREADY NAMED\n",
        "DATASET ALREADY NAMED\n",
        "DATASET ALREADY NAMED\n",
        "DATASET ALREADY NAMED\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>SBI_SEX</th>\n",
        "      <th>SBI_HOME_STATUS</th>\n",
        "      <th>SBI_AGE</th>\n",
        "      <th>SBI_ID</th>\n",
        "      <th>SBI_SALARY</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 4374.000000</td>\n",
        "      <td> 4366.000000</td>\n",
        "      <td> 4374.00000</td>\n",
        "      <td> 4374.000000</td>\n",
        "      <td>    4374.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>    1.375857</td>\n",
        "      <td>    1.252176</td>\n",
        "      <td>   54.59968</td>\n",
        "      <td> 3281.529950</td>\n",
        "      <td>   82953.490398</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>    0.484399</td>\n",
        "      <td>    0.486556</td>\n",
        "      <td>   13.93566</td>\n",
        "      <td> 2066.226094</td>\n",
        "      <td>   88008.137665</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>   19.00000</td>\n",
        "      <td>    5.000000</td>\n",
        "      <td>       1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>   45.00000</td>\n",
        "      <td> 1484.750000</td>\n",
        "      <td>   32087.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>   55.00000</td>\n",
        "      <td> 3054.500000</td>\n",
        "      <td>   67500.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>   64.00000</td>\n",
        "      <td> 5093.750000</td>\n",
        "      <td>  113661.250000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    3.000000</td>\n",
        "      <td>   95.00000</td>\n",
        "      <td> 7349.000000</td>\n",
        "      <td> 3390000.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "           SBI_SEX  SBI_HOME_STATUS     SBI_AGE       SBI_ID      SBI_SALARY\n",
        "count  4374.000000      4366.000000  4374.00000  4374.000000     4374.000000\n",
        "mean      1.375857         1.252176    54.59968  3281.529950    82953.490398\n",
        "std       0.484399         0.486556    13.93566  2066.226094    88008.137665\n",
        "min       1.000000         1.000000    19.00000     5.000000        1.000000\n",
        "25%       1.000000         1.000000    45.00000  1484.750000    32087.500000\n",
        "50%       1.000000         1.000000    55.00000  3054.500000    67500.000000\n",
        "75%       2.000000         1.000000    64.00000  5093.750000   113661.250000\n",
        "max       2.000000         3.000000    95.00000  7349.000000  3390000.000000"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#assemble input file fields from client and donor datasets based on matched IDs\n",
      "input_file_list = []\n",
      "\n",
      "for client_id, donor_id in matched_records.iteritems():\n",
      "    matched_record = donor_input[donor_input[donor_abbreviation + '_ID'] == donor_id]\n",
      "    matched_record[client_abbreviation + '_ID'] = client_id\n",
      "    input_file_list.append(matched_record)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_file = pd.concat(input_file_list)\n",
      "input_file.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>SBI_SEX</th>\n",
        "      <th>SBI_HOME_STATUS</th>\n",
        "      <th>SBI_AGE</th>\n",
        "      <th>SBI_STATE</th>\n",
        "      <th>SBI_ID</th>\n",
        "      <th>SBI_SALARY</th>\n",
        "      <th>PUMD_ID</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>4302</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 41</td>\n",
        "      <td> IL</td>\n",
        "      <td> 7011</td>\n",
        "      <td>      2</td>\n",
        "      <td> 2621444</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2483</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 59</td>\n",
        "      <td> TX</td>\n",
        "      <td> 3527</td>\n",
        "      <td> 181000</td>\n",
        "      <td> 2752522</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3289</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 27</td>\n",
        "      <td> KS</td>\n",
        "      <td> 5110</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 2621453</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3289</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 27</td>\n",
        "      <td> KS</td>\n",
        "      <td> 5110</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 2621454</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1411</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 31</td>\n",
        "      <td> MN</td>\n",
        "      <td> 1948</td>\n",
        "      <td>  15000</td>\n",
        "      <td> 2621455</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "      SBI_SEX  SBI_HOME_STATUS  SBI_AGE SBI_STATE  SBI_ID  SBI_SALARY  PUMD_ID\n",
        "4302        2                1       41        IL    7011           2  2621444\n",
        "2483        2                1       59        TX    3527      181000  2752522\n",
        "3289        2                2       27        KS    5110       17000  2621453\n",
        "3289        2                2       27        KS    5110       17000  2621454\n",
        "1411        2                1       31        MN    1948       15000  2621455"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#merge the donor data with the client data for comparison\n",
      "input_file = pd.merge(input_file, client_final, how = 'left', left_on=client_abbreviation + '_ID', right_on=client_abbreviation + '_ID')\n",
      "input_file.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>SBI_SEX</th>\n",
        "      <th>SBI_HOME_STATUS</th>\n",
        "      <th>SBI_AGE</th>\n",
        "      <th>SBI_STATE</th>\n",
        "      <th>SBI_ID</th>\n",
        "      <th>SBI_SALARY</th>\n",
        "      <th>PUMD_ID</th>\n",
        "      <th>PUMD_SEX</th>\n",
        "      <th>PUMD_SALARY</th>\n",
        "      <th>PUMD_AGE</th>\n",
        "      <th>PUMD_NEW_HOME_STATUS</th>\n",
        "      <th>PUMD_STATE_ABB</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 41</td>\n",
        "      <td> IL</td>\n",
        "      <td> 7011</td>\n",
        "      <td>      2</td>\n",
        "      <td> 2621444</td>\n",
        "      <td> 2</td>\n",
        "      <td>      0</td>\n",
        "      <td> 87</td>\n",
        "      <td> 2</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 59</td>\n",
        "      <td> TX</td>\n",
        "      <td> 3527</td>\n",
        "      <td> 181000</td>\n",
        "      <td> 2752522</td>\n",
        "      <td> 2</td>\n",
        "      <td> 181000</td>\n",
        "      <td> 39</td>\n",
        "      <td> 1</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 27</td>\n",
        "      <td> KS</td>\n",
        "      <td> 5110</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 2621453</td>\n",
        "      <td> 2</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 30</td>\n",
        "      <td> 2</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2</td>\n",
        "      <td> 2</td>\n",
        "      <td> 27</td>\n",
        "      <td> KS</td>\n",
        "      <td> 5110</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 2621454</td>\n",
        "      <td> 2</td>\n",
        "      <td>  17000</td>\n",
        "      <td> 30</td>\n",
        "      <td> 2</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> 31</td>\n",
        "      <td> MN</td>\n",
        "      <td> 1948</td>\n",
        "      <td>  15000</td>\n",
        "      <td> 2621455</td>\n",
        "      <td> 2</td>\n",
        "      <td>  15000</td>\n",
        "      <td> 30</td>\n",
        "      <td> 2</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "   SBI_SEX  SBI_HOME_STATUS  SBI_AGE SBI_STATE  SBI_ID  SBI_SALARY  PUMD_ID  \\\n",
        "0        2                1       41        IL    7011           2  2621444   \n",
        "1        2                1       59        TX    3527      181000  2752522   \n",
        "2        2                2       27        KS    5110       17000  2621453   \n",
        "3        2                2       27        KS    5110       17000  2621454   \n",
        "4        2                1       31        MN    1948       15000  2621455   \n",
        "\n",
        "   PUMD_SEX  PUMD_SALARY  PUMD_AGE  PUMD_NEW_HOME_STATUS PUMD_STATE_ABB  \n",
        "0         2            0        87                     2            NaN  \n",
        "1         2       181000        39                     1            NaN  \n",
        "2         2        17000        30                     2            NaN  \n",
        "3         2        17000        30                     2            NaN  \n",
        "4         2        15000        30                     2            NaN  "
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_file.to_csv(\"../data/outputs/cityblock.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict(input_file,'c',client_abbreviation,\"SEX\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "{'DT': 87.628267182962247,\n",
        " 'KNN': 61.936108422071634,\n",
        " 'LOG': 54.772507260406577,\n",
        " 'OVERALL': 77.189993029199911,\n",
        " 'SVM': 75.663117134559528}"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Home Status: \" + str(check_accuracy(input_file,'SBI_HOME_STATUS','PUMD_NEW_HOME_STATUS'))\n",
      "print \"Sex: \" + str(check_accuracy(input_file,'SBI_SEX','PUMD_SEX'))\n",
      "#print \"State: \" + str(check_accuracy(input_file,'SBI_STATE','PUMD_STATE_ABB'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Home Status: 80.4104477612\n",
        "Sex: 76.4925373134\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train, test = drop_and_split(input_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_vars = []\n",
      "y_var = \"\"\n",
      "for col in train.columns.values:\n",
      "    if col.find(\"AGE\") < 0 and col.find(\"STATE\") < 0:\n",
      "        x_vars.append(col)\n",
      "    elif col.find(\"AGE\") > 0 and col.find(client_abbreviation) > -1:\n",
      "        y_var = col\n",
      "\n",
      "print x_vars"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['SBI_SEX', 'SBI_HOME_STATUS', 'SBI_ID', 'SBI_SALARY', 'PUMD_ID', 'PUMD_SEX', 'PUMD_SALARY', 'PUMD_NEW_HOME_STATUS']\n"
       ]
      }
     ],
     "prompt_number": 265
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = train[['SBI_HOME_STATUS','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_SALARY', 'PUMD_NEW_HOME_STATUS']]\n",
      "y = train['PUMD_AGE']\n",
      "test_x = test[['SBI_HOME_STATUS','SBI_SEX','SBI_SALARY','PUMD_SEX','PUMD_SALARY','PUMD_NEW_HOME_STATUS']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SVR()\n",
      "clf.fit(x,y)\n",
      "test['AGE_PREDICT_SVM'] = clf.predict(test_x)\n",
      "#accuracy_score(test[['PUMD_AGE']],test[['AGE_PREDICT_SVM']])\n",
      "#check_accuracy(test,'PUMD_AGE','AGE_PREDICT_SVM')\n",
      "mean_squared_error(list(test.PUMD_AGE),list(test.AGE_PREDICT_SVM))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "317.16203633405485"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf2 = KNeighborsRegressor()\n",
      "clf2.fit(x,y)\n",
      "test['AGE_PREDICT_KNN'] = clf2.predict(test_x)\n",
      "#check_accuracy(test,'SBI_HOME_STATUS','HOME_STATUS_PREDICT_KNN')\n",
      "#accuracy_score(list(test.PUMD_NEW_HOME_STATUS),list(test['HOME_STATUS_PREDICT_KNN']))*100\n",
      "mean_squared_error(list(test.PUMD_AGE),list(test.AGE_PREDICT_KNN))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 198,
       "text": [
        "277.92888888888888"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#LIN\n",
      "clf3 = LinearRegression(fit_intercept=False)\n",
      "clf3.fit(x,y)\n",
      "test['AGE_PREDICT_LIN'] = clf3.predict(test_x)\n",
      "mean_squared_error(list(test.PUMD_AGE),list(test.AGE_PREDICT_LIN))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "394.91173591695343"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#write function to bring in a new SBI column, rename it, and replace NAs with either 0 or the median\n",
      "def add_SBI_col(col, colname, fillna):\n",
      "    if fillna == 'zero':\n",
      "        SBI_Input[colname] = SBI[col].fillna(0)\n",
      "    elif fillna == 'median':\n",
      "        SBI_Input[colname] = SBI[col].fillna(SBI[col].median())\n",
      "    elif fillna == 'pad':\n",
      "        SBI_Input[colname] = SBI[col].fillna(method='pad')\n",
      "    #print SBI_Input[colname].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#gender\n",
      "add_SBI_col('OCALC_10','SBI_Sex','pad')\n",
      "#employment status\n",
      "add_SBI_col('OCALC_6','SBI_Emp_Status','pad')\n",
      "#Retirement Assets\n",
      "add_SBI_col('FCALC_2','SBI_Ret_Assets','zero')\n",
      "SBI_Input['SBI_Ret_Assets'] = SBI_Input['SBI_Ret_Assets'] - SBI_Input['SBI_Ann_Val']\n",
      "#Liquid Assets\n",
      "add_SBI_col('NET_3','SBI_Liq_Assets','median')\n",
      "#Other Assets\n",
      "add_SBI_col('NET_1','SBI_Oth_Assets','median')\n",
      "SBI_Input['SBI_Oth_Assets'] = SBI_Input['SBI_Oth_Assets'] - SBI_Input['SBI_Ann_Val'] - SBI_Input['SBI_Ret_Assets'] - SBI_Input['SBI_Liq_Assets'] - SBI_Input['SBI_Home_Value']\n",
      "#Mortgages\n",
      "add_SBI_col('LIAB_1','SBI_Mortgage','zero')\n",
      "#Mortgage Payment\n",
      "add_SBI_col('G15_A','SBI_MortgagePayment','zero')\n",
      "#Rent\n",
      "add_SBI_col('G4C','SBI_RentPayment','zero')\n",
      "#All other liabilities\n",
      "add_SBI_col('NET_5','SBI_Oth_Liab','median')\n",
      "SBI_Input['SBI_Oth_Liab'] = SBI_Input['SBI_Oth_Liab'] - SBI_Input['SBI_Mortgage']\n",
      "SBI_Input.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>ID</th>\n",
        "      <th>SBI_Home_Value</th>\n",
        "      <th>SBI_Age</th>\n",
        "      <th>SBI_Income</th>\n",
        "      <th>SBI_Ann_Val</th>\n",
        "      <th>Joint_Decisions</th>\n",
        "      <th>Education</th>\n",
        "      <th>Financial_Confidence</th>\n",
        "      <th>SBI_Sex</th>\n",
        "      <th>SBI_Emp_Status</th>\n",
        "      <th>SBI_Ret_Assets</th>\n",
        "      <th>SBI_Liq_Assets</th>\n",
        "      <th>SBI_Oth_Assets</th>\n",
        "      <th>SBI_Mortgage</th>\n",
        "      <th>SBI_MortgagePayment</th>\n",
        "      <th>SBI_RentPayment</th>\n",
        "      <th>SBI_Oth_Liab</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 4374.000000</td>\n",
        "      <td>    4374.000000</td>\n",
        "      <td> 4374.00000</td>\n",
        "      <td>    4374.000000</td>\n",
        "      <td>    4374.000000</td>\n",
        "      <td> 4372.000000</td>\n",
        "      <td> 4360.000000</td>\n",
        "      <td> 4335.000000</td>\n",
        "      <td> 4374.000000</td>\n",
        "      <td> 4374.000000</td>\n",
        "      <td>    4374.000000</td>\n",
        "      <td>     4374.000000</td>\n",
        "      <td>     4374.000000</td>\n",
        "      <td>   4374.000000</td>\n",
        "      <td>   4374.000000</td>\n",
        "      <td> 4374.000000</td>\n",
        "      <td>    4374.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 3281.529950</td>\n",
        "      <td>  221229.618884</td>\n",
        "      <td>   54.59968</td>\n",
        "      <td>   82953.490398</td>\n",
        "      <td>    8574.608434</td>\n",
        "      <td>    1.488335</td>\n",
        "      <td>    4.952523</td>\n",
        "      <td>    2.062745</td>\n",
        "      <td>    1.375857</td>\n",
        "      <td>    2.969364</td>\n",
        "      <td>  191284.748905</td>\n",
        "      <td>   180770.969136</td>\n",
        "      <td>   184634.178555</td>\n",
        "      <td>  73044.042981</td>\n",
        "      <td>    886.012574</td>\n",
        "      <td>  167.951760</td>\n",
        "      <td>   48470.139689</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 2066.226094</td>\n",
        "      <td>  256418.761015</td>\n",
        "      <td>   13.93566</td>\n",
        "      <td>   88008.137665</td>\n",
        "      <td>  116327.590793</td>\n",
        "      <td>    0.499921</td>\n",
        "      <td>    1.673318</td>\n",
        "      <td>    0.768996</td>\n",
        "      <td>    0.484399</td>\n",
        "      <td>    2.717506</td>\n",
        "      <td>  373801.610351</td>\n",
        "      <td>   603896.524559</td>\n",
        "      <td>   939035.014317</td>\n",
        "      <td> 115149.716634</td>\n",
        "      <td>   9640.661436</td>\n",
        "      <td>  452.101836</td>\n",
        "      <td>  115175.709475</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>    5.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>   19.00000</td>\n",
        "      <td>       1.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>        0.000000</td>\n",
        "      <td>   -55000.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td> 1484.750000</td>\n",
        "      <td>   10000.000000</td>\n",
        "      <td>   45.00000</td>\n",
        "      <td>   32087.500000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    4.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>     3500.000000</td>\n",
        "      <td>     9000.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td> 3054.500000</td>\n",
        "      <td>  170000.000000</td>\n",
        "      <td>   55.00000</td>\n",
        "      <td>   67500.000000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    5.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>   50000.000000</td>\n",
        "      <td>    22400.000000</td>\n",
        "      <td>    30000.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    9882.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 5093.750000</td>\n",
        "      <td>  300000.000000</td>\n",
        "      <td>   64.00000</td>\n",
        "      <td>  113661.250000</td>\n",
        "      <td>       0.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    6.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    7.000000</td>\n",
        "      <td>  210000.000000</td>\n",
        "      <td>   125741.000000</td>\n",
        "      <td>   102899.500000</td>\n",
        "      <td> 116000.000000</td>\n",
        "      <td>   1084.500000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>   45297.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 7349.000000</td>\n",
        "      <td> 5000000.000000</td>\n",
        "      <td>   95.00000</td>\n",
        "      <td> 3390000.000000</td>\n",
        "      <td> 7199999.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    9.000000</td>\n",
        "      <td>    4.000000</td>\n",
        "      <td>    2.000000</td>\n",
        "      <td>    8.000000</td>\n",
        "      <td> 5030000.000000</td>\n",
        "      <td> 21310100.000000</td>\n",
        "      <td> 30187857.000000</td>\n",
        "      <td> 999999.000000</td>\n",
        "      <td> 574076.000000</td>\n",
        "      <td> 5500.000000</td>\n",
        "      <td> 2074000.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "                ID  SBI_Home_Value     SBI_Age      SBI_Income  \\\n",
        "count  4374.000000     4374.000000  4374.00000     4374.000000   \n",
        "mean   3281.529950   221229.618884    54.59968    82953.490398   \n",
        "std    2066.226094   256418.761015    13.93566    88008.137665   \n",
        "min       5.000000        0.000000    19.00000        1.000000   \n",
        "25%    1484.750000    10000.000000    45.00000    32087.500000   \n",
        "50%    3054.500000   170000.000000    55.00000    67500.000000   \n",
        "75%    5093.750000   300000.000000    64.00000   113661.250000   \n",
        "max    7349.000000  5000000.000000    95.00000  3390000.000000   \n",
        "\n",
        "          SBI_Ann_Val  Joint_Decisions    Education  Financial_Confidence  \\\n",
        "count     4374.000000      4372.000000  4360.000000           4335.000000   \n",
        "mean      8574.608434         1.488335     4.952523              2.062745   \n",
        "std     116327.590793         0.499921     1.673318              0.768996   \n",
        "min          0.000000         1.000000     1.000000              1.000000   \n",
        "25%          0.000000         1.000000     4.000000              2.000000   \n",
        "50%          0.000000         1.000000     5.000000              2.000000   \n",
        "75%          0.000000         2.000000     6.000000              2.000000   \n",
        "max    7199999.000000         2.000000     9.000000              4.000000   \n",
        "\n",
        "           SBI_Sex  SBI_Emp_Status  SBI_Ret_Assets   SBI_Liq_Assets  \\\n",
        "count  4374.000000     4374.000000     4374.000000      4374.000000   \n",
        "mean      1.375857        2.969364   191284.748905    180770.969136   \n",
        "std       0.484399        2.717506   373801.610351    603896.524559   \n",
        "min       1.000000        1.000000        0.000000         0.000000   \n",
        "25%       1.000000        1.000000        0.000000      3500.000000   \n",
        "50%       1.000000        1.000000    50000.000000     22400.000000   \n",
        "75%       2.000000        7.000000   210000.000000    125741.000000   \n",
        "max       2.000000        8.000000  5030000.000000  21310100.000000   \n",
        "\n",
        "        SBI_Oth_Assets   SBI_Mortgage  SBI_MortgagePayment  SBI_RentPayment  \\\n",
        "count      4374.000000    4374.000000          4374.000000      4374.000000   \n",
        "mean     184634.178555   73044.042981           886.012574       167.951760   \n",
        "std      939035.014317  115149.716634          9640.661436       452.101836   \n",
        "min      -55000.000000       0.000000             0.000000         0.000000   \n",
        "25%        9000.000000       0.000000             0.000000         0.000000   \n",
        "50%       30000.000000       0.000000             0.000000         0.000000   \n",
        "75%      102899.500000  116000.000000          1084.500000         0.000000   \n",
        "max    30187857.000000  999999.000000        574076.000000      5500.000000   \n",
        "\n",
        "         SBI_Oth_Liab  \n",
        "count     4374.000000  \n",
        "mean     48470.139689  \n",
        "std     115175.709475  \n",
        "min          0.000000  \n",
        "25%          0.000000  \n",
        "50%       9882.000000  \n",
        "75%      45297.500000  \n",
        "max    2074000.000000  "
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#clean up SBI file\n",
      "#Fields: SBI ID, HH Income, Age, Gender, Employment Status, Liquid Asset Balance, Other Investment Balance, Home Value, Retirement Assets, Variable Annuity Balance\n",
      "#donor_input = donor_data[['ID','OCALC_4','OCALC_10','OCALC_27','STATE','G4']]\n",
      "#donor_input = donor_input.rename(columns = {'ID':'SBI_ID','OCALC_4':'SBI_AGE','OCALC_27':'SBI_SALARY','OCALC_10':'SBI_SEX',\n",
      "                                      #  'STATE':'SBI_STATE','G4':'SBI_HOME_STATUS'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 272
    }
   ],
   "metadata": {}
  }
 ]
}